{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "mpl.rcParams['font.serif'] = ['SimHei']\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "p = sns.color_palette()\n",
    "sns.set_style(\"darkgrid\",{\"font.sans-serif\":['simhei', 'Arial']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_csv(\"../feature/特征汇总_20170119_D.csv\",encoding=\"gb2312\") # 注意自己数据路径\n",
    "#dataset=trains[:]\n",
    "beifen=0\n",
    "trains=0\n",
    "dataset.drop('放款时间_x',axis=1, inplace=True)\n",
    "dataset.drop('放款时间_y',axis=1, inplace=True)\n",
    "dataset.drop('放款时间_x.1',axis=1, inplace=True)\n",
    "dataset.drop('放款时间_y.1',axis=1, inplace=True)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#dataset=trains[:]\n",
    "#检测重复特征名称,并删除\n",
    "names=dataset.columns.tolist()\n",
    "print(len(names))\n",
    "print(len(set(names)))\n",
    "print(\"dataset大小：\")\n",
    "print(dataset.shape)\n",
    "myset = set(names)\n",
    "for item in myset:\n",
    "    #dataset[item] = MinMaxScaler().fit_transform(dataset[item])\n",
    "    if names.count(item)>1:\n",
    "        print(item)\n",
    "        dataset=dataset.drop(item,axis=1)\n",
    "print(\"新dataset大小：\")\n",
    "print(dataset.shape)\n",
    "#dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 20))\n",
    "\n",
    "\n",
    "ax1 = fig.add_subplot(3, 2, 1)\n",
    "ax1=sns.barplot(职业分布.index, 职业分布.逾期/职业分布.总数, alpha=0.8, color=p[0], label='train')\n",
    "ax1.legend()\n",
    "#ax1.set_title(u'职业分布情况') \n",
    "ax1.set_xlabel(u'用户职业')\n",
    "ax1.set_ylabel(u'逾期用户比例')\n",
    "\n",
    "ax2 = fig.add_subplot(3, 2, 2)\n",
    "ax2=sns.barplot(性别分布.index, 性别分布.逾期/性别分布.总数, alpha=0.8, color=p[1], label='train')\n",
    "ax2.legend()\n",
    "#ax2.set_title(u'性别分布情况') \n",
    "ax2.set_xlabel(u'用户性别')\n",
    "ax2.set_ylabel(u'逾期用户比例')\n",
    "\n",
    "ax3 = fig.add_subplot(3, 2, 3)\n",
    "ax3=sns.barplot(教育程度分布.index, 教育程度分布.逾期/教育程度分布.总数, alpha=0.8, color=p[2], label='train')\n",
    "ax3.legend()\n",
    "#ax3.set_title(u'教育程度分布') \n",
    "ax3.set_xlabel(u'教育程度')\n",
    "ax3.set_ylabel(u'逾期用户比例')\n",
    "\n",
    "ax4 = fig.add_subplot(3, 2, 4)\n",
    "ax4=sns.barplot(用户婚姻状态.index, 用户婚姻状态.逾期/用户婚姻状态.总数, alpha=0.8, color=p[3], label='train')\n",
    "ax4.legend()\n",
    "#ax4.set_title(u'用户婚姻状态') \n",
    "ax4.set_xlabel(u'用户婚姻状态')\n",
    "ax4.set_ylabel(u'逾期用户比例')\n",
    "\n",
    "ax5 = fig.add_subplot(3, 2, 5)\n",
    "ax5=sns.barplot(用户户口类型.index, 用户户口类型.逾期/用户户口类型.总数, alpha=0.8, color=p[4], label='train')\n",
    "ax5.legend()\n",
    "#ax5.set_title(u'用户户口类型') \n",
    "ax5.set_xlabel(u'用户户口类型')\n",
    "ax5.set_ylabel(u'逾期用户比例')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=t1['时间'].apply(lambda x:np.unique(x).size)\n",
    "x1=t1['时间'].agg({'t1' : 'count'})\n",
    "x1['x1']=x\n",
    "\n",
    "x=t2['时间'].apply(lambda x:np.unique(x).size)\n",
    "x2=t2['时间'].agg({'t2' : 'count'})\n",
    "x2['x2']=x\n",
    "\n",
    "x=t3['时间'].apply(lambda x:np.unique(x).size)\n",
    "x3=t3['时间'].agg({'t3' : 'count'})\n",
    "x3['x3']=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l0_x1=d[(d['标签']==0)].groupby(\"x1\",as_index=False)\n",
    "l1_x1=d[(d['标签']==1)].groupby(\"x1\",as_index=False)\n",
    "\n",
    "l0_x1=l0_x1['x1'].agg({'l0_x1' : 'count'})#标签为0\n",
    "l1_x1=l1_x1['x1'].agg({'l1_x1' : 'count'})#标签为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(d, hue=\"标签\", size=5).map(plt.scatter, \"x1\", \"标签\").add_legend()\n",
    "xxx=d[['标签','x1','x2','x3']]\n",
    "sns.pairplot(xxx, hue=\"标签\", size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(trains, hue=\"标签\", size=5).map(plt.scatter, \"放款前账单还款差额\", \"标签\").add_legend()\n",
    "xxx=trains[['标签','放款前账单还款差额','放款后账单还款差额','放款前账单最大值还款差额(去重)','放款后账单最大值还款差额(去重)']]\n",
    "sns.pairplot(xxx, hue=\"标签\", size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p1=(1,2,3,4,9,10,11,12,5,6,7,8)\n",
    "y_t1=(0,0,0,0,0,0,0,0,1,1,1,1)\n",
    "\n",
    "y_p2=(1,2,3,5,8,10,11,12,4,6,7,9)\n",
    "y_t2=(0,0,0,0,0,0,0,0,1,1,1,1)\n",
    "\n",
    "y_p3=(1,2,4,5,8,9,11,12,3,6,7,10)\n",
    "y_t3=(0,0,0,0,0,0,0,0,1,1,1,1)\n",
    "\n",
    "y_p4=(1,2,4,6,7,9,11,12,3,5,8,10)\n",
    "y_t4=(0,0,0,0,0,0,0,0,1,1,1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_detail_train = pd.read_csv('D:/data/DataCastle/train/bank_detail_train.txt',\n",
    "                                    header = None)\n",
    "bank_detail_test = pd.read_csv('D:/data/DataCastle/test/bank_detail_test.txt',\n",
    "                                    header = None)\n",
    "col_names = ['userid', 'tm_encode', 'trade_type', 'trade_amount', 'salary_tag']\n",
    "bank_detail_train.columns = col_names\n",
    "bank_detail_test.columns = col_names\n",
    "bank_detail = pd.concat([bank_detail_train, bank_detail_test])\n",
    "# 在该数据集中，一个用户对应多条记录，这里我们采用对每个用户每种交易类型取均值进行聚合\n",
    "bank_detail_n = (bank_detail.loc[:, ['userid', 'trade_type', 'trade_amount', 'tm_encode']]).groupby(['userid', 'trade_type']).mean()\n",
    "# 重塑数据集，并设置字段（列）名称\n",
    "bank_detail_n = bank_detail_n.unstack()\n",
    "bank_detail_n.columns = ['income', 'outcome', 'income_tm', 'outcome_tm']\n",
    "bank_detail_n.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browse_history_train = pd.read_csv('D:/data/DataCastle/train/browse_history_train.txt',\n",
    "                                       header = None)\n",
    "browse_history_test = pd.read_csv('D:/data/DataCastle/test/browse_history_test.txt',\n",
    "                                       header = None)\n",
    "col_names = ['userid', 'tm_encode_2', 'browse_data', 'browse_tag']\n",
    "browse_history_train.columns = col_names\n",
    "browse_history_test.columns = col_names\n",
    "browse_history = pd.concat([browse_history_train, browse_history_test])\n",
    "# 这里采用计算每个用户总浏览行为次数进行聚合\n",
    "browse_history_count = browse_history.loc[:, ['userid', 'browse_data']].groupby(['userid']).sum()\n",
    "print browse_history_count.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bill_detail_train = pd.read_csv('D:/data/DataCastle/train/bill_detail_train.txt',\n",
    "                                       header = None)\n",
    "bill_detail_test = pd.read_csv('D:/data/DataCastle/test/bill_detail_test.txt',\n",
    "                                       header = None)\n",
    "col_names = ['userid', 'tm_encode_3', 'bank_id', 'prior_account', 'prior_repay',\n",
    "             'credit_limit', 'account_balance', 'minimun_repay', 'consume_count',\n",
    "             'account', 'adjust_account', 'circulated_interest', 'avaliable_balance',\n",
    "             'cash_limit', 'repay_state']\n",
    "bill_detail_train.columns = col_names\n",
    "bill_detail_test.columns = col_names\n",
    "bill_detail = pd.concat([bill_detail_train, bill_detail_test])\n",
    "bill_detail_mean = bill_detail.groupby(['userid']).mean()\n",
    "bill_detail_mean.drop('bank_id',\n",
    "                      axis = 1,\n",
    "                      inplace = True)\n",
    "print bill_detail_mean.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_data = user_info.join(bank_detail_n, how = 'outer')\n",
    "loan_data = loan_data.join(bill_detail_mean, how = 'outer')\n",
    "loan_data = loan_data.join(browse_history_count, how = 'outer')\n",
    "loan_data = loan_data.join(loan_time, how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填补缺失值\n",
    "loan_data = loan_data.fillna(0.0)\n",
    "print loan_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对性别、职业等因子变量，构造其哑变量\n",
    "category_col = ['sex', 'occupation', 'education', 'marriage', 'household']\n",
    "def set_dummies(data, colname):\n",
    "    for col in colname:\n",
    "        data[col] = data[col].astype('category')\n",
    "        dummy = pd.get_dummies(data[col])\n",
    "        dummy = dummy.add_prefix('{}#'.format(col))\n",
    "        data.drop(col,\n",
    "                  axis = 1,\n",
    "                  inplace = True)\n",
    "        data = data.join(dummy)\n",
    "    return data\n",
    "loan_data = set_dummies(loan_data, category_col)\n",
    "\n",
    "# overdue_train，这是我们模型所要拟合的目标\n",
    "target = pd.read_csv('D:/data/DataCastle/train/overdue_train.txt',\n",
    "                         header = None)\n",
    "target.columns = ['userid', 'label']\n",
    "target.index = target['userid']\n",
    "target.drop('userid',\n",
    "            axis = 1,\n",
    "            inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#precidt modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里用Logistic回归\n",
    "lr_model = LogisticRegression(C = 1.0,\n",
    "                              penalty = 'l2')\n",
    "lr_model.fit(train_X, train_y)\n",
    "# 给出交叉验证集的预测结果，评估准确率、召回率、F1值\n",
    "pred_test = lr_model.predict(test_X)\n",
    "print classification_report(test_y, pred_test)\n",
    "# 输出测试集用户逾期还款概率，predict_proba会输出两个概率，取‘1’的概率\n",
    "pred = lr_model.predict_proba(test)\n",
    "result = pd.DataFrame(pred)\n",
    "result.index = test.index\n",
    "result.columns = ['0', 'probability']\n",
    "result.drop('0',\n",
    "            axis = 1,\n",
    "            inplace = True)\n",
    "result.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
